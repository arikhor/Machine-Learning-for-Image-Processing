{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Arik Horodniceanu, PID A53285765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 285 MLIP Assignment 1 - Backpropagation  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load in module MNISTtools:\n",
      "\n",
      "load(dataset='training', path=None)\n",
      "    Import either the training or testing MNIST data set.\n",
      "    It returns a pair with the first element being the collection of\n",
      "    images stacked in columns and the second element being a vector\n",
      "    of corresponding labels from 0 to 9.\n",
      "    \n",
      "    Arguments:\n",
      "        dataset (string, optional): either \"training\" or \"testing\".\n",
      "            (default: \"training\")\n",
      "        path (string, optional): the path pointing to the MNIST dataset\n",
      "            If path=None, it looks succesively for the dataset at:\n",
      "            '/datasets/MNIST' and './MNIST'. (default: None)\n",
      "    \n",
      "    Example:\n",
      "        x, lbl = load(dataset=\"testing\", path=\"/Folder/for/MNIST\")\n",
      "\n",
      "Help on function show in module MNISTtools:\n",
      "\n",
      "show(image)\n",
      "    Render a given MNIST image provided as a column vector.\n",
      "    \n",
      "    Arguments:\n",
      "        image (array): an array of shape (28*28) or (28, 28) representing a\n",
      "            grey level image of size 28 x 28. Values are expected to be in the\n",
      "            range [0, 1].\n",
      "    \n",
      "    Example:\n",
      "        x, lbl = load(dataset=\"training\", path=\"/datasets/MNIST\")\n",
      "        show(x[:, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MNISTtools.load)\n",
    "help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ltrain=MNISTtools.load(dataset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of xtrain is 784x60000. The shape of ltrain is 60000x1.\n",
    "The training dataset has 60,000 items. The feature dimension is 28x28, or\n",
    "784x1 in the column stacked format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADLRJREFUeJzt3W+oXPWdx/HPx258YBJjbK6XYLV3V/KkFJosg6xWF6W0uIL/nvgPSwLS+KDCigX/PmgeiMhSLT5YhNiE3hR1W1AxoGTrJgXpk9BJiEk0trblynpzvZmgcA2EtNHvPpiTcjfeOTPOnJkz6ff9gmHOnO85OV+Ofu6Zc34zZxwRApDPeXU3AKAehB9IivADSRF+ICnCDyRF+IGkagm/7Rts/972H20/UkcPndiesX3I9gHbzZp72W77mO3Di+ZdbPtN2+8Xz6vHqLcttmeLfXfA9o019XaZ7d/Yftf2O7b/vZhf674r6auW/eZRj/Pb/oqkP0j6rqQPJf1O0l0R8e5IG+nA9oykRkQcH4Ne/lXSCUk7IuKbxbz/kPRxRDxV/OFcHREPj0lvWySdiIifjLqfs3pbK2ltROy3vVLSPkm3StqkGvddSV+3q4b9VseR/0pJf4yIP0fEXyT9l6Rbauhj7EXEW5I+Pmv2LZKmi+lptf/nGbkOvY2FiJiLiP3F9KeSjki6VDXvu5K+alFH+C+V9L+LXn+oGnfAEkLSr23vs7257maWMBkRc8X0R5Im62xmCffbPlicFtRySrKY7SlJGyTt1Rjtu7P6kmrYb1zw+6JrIuKfJf2bpB8Wb2/HUrTP2cbp89nPSbpC0npJc5KerrMZ2yskvSzpgYhYWFyrc98t0Vct+62O8M9KumzR668V88ZCRMwWz8ckvar2aco4mS/OHc+cQx6ruZ+/iYj5iPgsIj6X9Lxq3He2l6kdsBci4pVidu37bqm+6tpvdYT/d5LW2f5H2+dLulPSzhr6+ALby4sLMbK9XNL3JB0uX2vkdkraWExvlPRajb38P2eCVbhNNe0725a0TdKRiHhmUanWfdepr9r2W0SM/CHpRrWv+P9J0uN19NChr3+S9HbxeKfu3iS9pPbbwL+qfW3kXklflbRb0vuS/kfSxWPU2y8kHZJ0UO2gra2pt2vUfkt/UNKB4nFj3fuupK9a9tvIh/oAjAcu+AFJEX4gKcIPJEX4gaQIP5BUreEf04/PShrf3sa1L4ne+lVXb3Uf+cf2P4jGt7dx7Uuit36lDD+Amgz0IR/bN0h6VtJXJP0sIp4qW37NmjUxNTX1t9etVksTExN9b3+YxrW3ce1Lord+VdnbzMyMjh8/7l6W/Yd+N1LclOM/teimHLZ3RslNOaamptRs1npzHODvWqPR6HnZQd72c1MO4Bw2SPjH/aYcAEoM/YKf7c22m7abrVZr2JsD0KNBwt/TTTkiYmtENCKiMa4XXICMBgn/2N6UA0B3fV/tj4jTtu+X9N9qD/Vtj4h3KusMwFD1HX5Jiog3JL1RUS8ARohP+AFJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSGugnum3PSPpU0meSTkdEo4qmAAzfQOEvXB8Rxyv4dwCMEG/7gaQGDX9I+rXtfbY3V9EQgNEY9G3/NRExa/sSSW/afi8i3lq8QPFHYbMkXX755QNuDkBVBjryR8Rs8XxM0quSrlxima0R0YiIxsTExCCbA1ChvsNve7ntlWemJX1P0uGqGgMwXIO87Z+U9KrtM//OixGxq5KuAAxd3+GPiD9L+laFvQAYIYb6gKQIP5AU4QeSIvxAUoQfSKqKL/bgHBYRpfUTJ06U1nftKh/d3bFjR8fa22+/XbruoUOHSuurVq0qraMcR34gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIpx/r8DCwsLHWt79uwpXXfbtm2l9ddff72vnnqxfPny0vqyZcuGtm1w5AfSIvxAUoQfSIrwA0kRfiApwg8kRfiBpBjnHwNHjx4trT/55JOl9bKx+lOnTpWuu27dutL6li1bSuunT58urT/xxBMda3fccUfpuhdccEFpHYPhyA8kRfiBpAg/kBThB5Ii/EBShB9IivADSTHOX4H33nuvtH7zzTeX1mdnZ0vrJ0+eLK0/+uijHWubNm0qXXdqaqq03u079d16Lxvn37BhQ+m6GK6uR37b220fs3140byLbb9p+/3iefVw2wRQtV7e9v9c0g1nzXtE0u6IWCdpd/EawDmka/gj4i1JH581+xZJ08X0tKRbK+4LwJD1e8FvMiLmiumPJE12WtD2ZttN281Wq9Xn5gBUbeCr/dH+pceOv/YYEVsjohERjYmJiUE3B6Ai/YZ/3vZaSSqej1XXEoBR6Df8OyVtLKY3SnqtmnYAjErXcX7bL0m6TtIa2x9K+rGkpyT9yva9kj6QdPswmxx3n3zySWn92muvLa2vWLGitH7PPfeU1huNRsea7dJ169Ttvv0Yrq7hj4i7OpS+U3EvAEaIj/cCSRF+ICnCDyRF+IGkCD+QFF/prcBVV101UP1c9vDDD/e97p133llhJ/iyOPIDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKM82MgMzMzdbeAPnHkB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkGOfHUF1//fUda+eff/4IO8HZOPIDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKM86PUwsJCaX3fvn2l9U2bNnWsnXcex546dd37trfbPmb78KJ5W2zP2j5QPG4cbpsAqtbLn96fS7phifk/jYj1xeONatsCMGxdwx8Rb0n6eAS9ABihQU667rd9sDgtWN1pIdubbTdtN1ut1gCbA1ClfsP/nKQrJK2XNCfp6U4LRsTWiGhERGNiYqLPzQGoWl/hj4j5iPgsIj6X9LykK6ttC8Cw9RV+22sXvbxN0uFOywIYT13H+W2/JOk6SWtsfyjpx5Kus71eUkiakXTfEHtEjfbs2VNaP3XqVGn9wQcfrLIdVKhr+CPiriVmbxtCLwBGiI9YAUkRfiApwg8kRfiBpAg/kBRf6UWp3bt3l9a7fS33kksuqbIdVIgjP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kxTg/Sh09erS0fvXVV5fWV61aVWU7qBBHfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iql5/ovkzSDkmTav8k99aIeNb2xZJ+KWlK7Z/pvj0iPhleqxiGbj+xvWvXrtL6TTfdVGU7GKFejvynJf0oIr4h6V8k/dD2NyQ9Iml3RKyTtLt4DeAc0TX8ETEXEfuL6U8lHZF0qaRbJE0Xi01LunVYTQKo3pc657c9JWmDpL2SJiNirih9pPZpAYBzRM/ht71C0suSHoiIhcW1iAi1rwcstd5m203bzVarNVCzAKrTU/htL1M7+C9ExCvF7Hnba4v6WknHllo3IrZGRCMiGhMTE1X0DKACXcNv25K2SToSEc8sKu2UtLGY3ijpterbAzAsvdy6+9uSvi/pkO0DxbzHJD0l6Ve275X0gaTbh9Mihmnv3r2l9ZMnT5bWH3rooSrbwQh1DX9E/FaSO5S/U207AEaFT/gBSRF+ICnCDyRF+IGkCD+QFOEHkuInupObnp7uvlCJyUm+0nGu4sgPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kxzo9SF110UWn9wgsvHFEnqBpHfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IinH+5Pbv319a7/YrSytXrqyyHYwQR34gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSKrrOL/tyyTtkDQpKSRtjYhnbW+R9ANJrWLRxyLijWE1iv68+OKLpfUDBw6U1h9//PEq28EY6eVDPqcl/Sgi9tteKWmf7TeL2k8j4ifDaw/AsHQNf0TMSZorpj+1fUTSpcNuDMBwfalzfttTkjZI2lvMut/2Qdvbba+uuDcAQ9Rz+G2vkPSypAciYkHSc5KukLRe7XcGT3dYb7Ptpu1mq9VaahEANegp/LaXqR38FyLiFUmKiPmI+CwiPpf0vKQrl1o3IrZGRCMiGt2+JAJgdLqG37YlbZN0JCKeWTR/7aLFbpN0uPr2AAxLL1f7vy3p+5IO2T4zLvSYpLtsr1d7+G9G0n1D6RADmZ+fH2j9u+++u6JOMG56udr/W0leosSYPnAO4xN+QFKEH0iK8ANJEX4gKcIPJEX4gaQcESPbWKPRiGazObLtAdk0Gg01m82lhua/gCM/kBThB5Ii/EBShB9IivADSRF+ICnCDyQ10nF+2y1JH4xsg0A+X4+Inm6ZNdLwAxgfvO0HkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeS+j+uaNxGA6PvtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNISTtools.show(xtrain[:,42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would've figured that to be a 1, but I guess it's a 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "print(type(xtrain))\n",
    "print(type(xtrain[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range is 0-255, xtrain is an ndarray of uint8 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    normalized=x.astype(np.float32)\n",
    "    normalized-=127.5\n",
    "    normalized/=127.5\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=normalize_MNIST_images(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "print(type(xtrain))\n",
    "print(type(xtrain[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's an ndarray of float32 type, as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl,np.arange(lbl.size)] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain=label2onehot(ltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(dtrain[:,42])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed there is a 1 in the 7'th position only, rest are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(ltrain==onehot2label(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    y=np.exp(a-a.max(axis=0))\n",
    "    y/=y.sum(axis=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}&\\frac{dg(\\textbf{a})_i}{da_i}=\\frac{d}{da_i} \\frac{exp(a_i - M)}{\\sum_{j=0}^{10} exp(a_j -M)}\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the quotient rule for derivatives and the facts that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}&\\frac{dexp(a_j - M)}{da_i} = 0 , i\\neq j \\\\ &\\frac{dexp(a_i - M)}{da_i} = exp(a_i - M)\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}&\\frac{d}{da_i} \\frac{exp(a_i - M)}{\\sum_{j=1}^{10} exp(a_j -M)} = \\\\\n",
    "&\\frac{exp(a_i - M)*\\sum_{j=0}^{10} exp(a_j -M) - exp(a_i - M)* exp(a_i - M)}{(\\sum_{j=1}^{10} exp(a_j -M))^2} = \\\\\n",
    "&= \\frac{exp(a_i - M)}{(\\sum_{j=1}^{10} exp(a_j -M))^2} - \\frac{(exp(a_i - M))^2}{(\\sum_{j=1}^{10} exp(a_j -M))^2} = \\\\\n",
    "&g(\\textbf{a})_i-g(\\textbf{a})_i^2 = g(\\textbf{a})_i(1-g(\\textbf{a})_i) .\\square\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to section 8, for $i\\neq j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}&\\frac{dg(\\textbf{a})_i}{da_j}=\\frac{d}{da_j} \\frac{exp(a_i - M)}{\\sum_{j=1}^{10} exp(a_j -M)} = \\\\\n",
    "&=\\frac{0-exp(a_i - M) exp(a_j - M)}{(\\sum_{j=1}^{10} exp(a_j -M))^2} = \\\\\n",
    "&=-\\frac{exp(a_i - M)}{\\sum_{j=1}^{10} exp(a_j -M)} \\frac{exp(a_j - M)}{\\sum_{j=1}^{10} exp(a_j -M)} = \\\\\n",
    "&= -g(\\textbf{a})_ig(\\textbf{a})_j . \\square\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, for $i\\neq j$:$$\\frac{dg(\\textbf{a})_i}{da_j} = \\frac{dg(\\textbf{a})_j}{da_i}  = -g(\\textbf{a})_ig(\\textbf{a})_j$$from section 9. Thus, the Jacobian of softmax is symmetric since:$$(\\frac{dg(\\textbf{a})}{d\\textbf{a}})_{i,j} = (\\frac{dg(\\textbf{a})}{d\\textbf{a}})_{j,i}$$\n",
    "\n",
    "So, multiplying directly and using the results of section 8 and 9 we get:\n",
    "$$\\boldsymbol{\\delta} = [\\frac{dg(\\textbf{a})}{d\\textbf{a}}]^T \\times \\textbf{e} = \\begin{pmatrix} \\frac{dg(\\boldsymbol{a})_1}{da_1}*e_1 + ... +  \\frac{dg(\\boldsymbol{a})_{1}}{da_{10}}*e_{10}\n",
    "\\\\ ...  \n",
    "\\\\ \\frac{dg(\\boldsymbol{a})_{10}}{da_1}*e_1 + ... +  \\frac{dg(\\boldsymbol{a})_{10}}{da_{10}}*e_{10}\n",
    "\\end{pmatrix}$$ \n",
    "$$= \\begin{pmatrix} \\frac{dg(\\boldsymbol{a})_1}{da_1}*e_1\n",
    "\\\\ ...  \n",
    "\\\\ \\frac{dg(\\boldsymbol{a})_{10}}{da_{10}}*e_{10}\n",
    "\\end{pmatrix} +\\begin{pmatrix} \\frac{dg(\\boldsymbol{a})_1}{da_2}*e_2 + ... +  \\frac{dg(\\boldsymbol{a})_{1}}{da_{10}}*e_{10}\n",
    "\\\\ ...  \n",
    "\\\\ \\frac{dg(\\boldsymbol{a})_{10}}{da_1}*e_1 + ... +  \\frac{dg(\\boldsymbol{a})_{10}}{da_{9}}*e_{9}\n",
    "\\end{pmatrix}=$$ \n",
    "$$\\\\ g(\\textbf{a}) \\otimes \\textbf{e} -<g(\\textbf{a}),\\textbf(e)>g(\\textbf{a}) . \\square$$ \n",
    "Where $e_i$ is the element in the $i$th position in the vector $\\textbf{e}, i={1,...,10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a,e):\n",
    "    sftmax=softmax(a)\n",
    "    tmp=sftmax*e\n",
    "    return tmp-tmp.sum(axis=0)*sftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.951952269743215e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 # finite difference step\n",
    "a = np.random.randn(10, 200) # random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = softmaxp(a, e)\n",
    "diff_approx = (softmax(a+eps*e)-softmax(a))/eps # Using the given definition\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, 'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    return a*(a>0)\n",
    "def relup(a, e):\n",
    "    return e*(a>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    a1 = W1.dot(x) + b1\n",
    "    a2 = W2.dot(relu(a1)) + b2\n",
    "    y=softmax(a2)\n",
    "    return y\n",
    "\n",
    "yinit = forwardprop_shallow(xtrain, netinit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2797001963013825 should be around .26\n"
     ]
    }
   ],
   "source": [
    "def eval_loss(y, d):\n",
    "    return -np.sum(d*np.log(y))/d.size # according to the definition\n",
    "\n",
    "print(eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9279833333333334\n"
     ]
    }
   ],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    return np.sum(np.not_equal(np.argmax(y,axis=0),lbl))/lbl.size\n",
    "\n",
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First time has large error rate, we'll see it improves with more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly $(\\nabla_{\\textbf{y}}E)_i = \\nabla_{y_i}(-d_i logy_i) = -\\frac{d_i}{y_i},\\forall i\\in [1,10].$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    # Same as before here for forward pass\n",
    "    a1 = W1.dot(x) + b1  \n",
    "    a2 = W2.dot(relu(a1)) + b2\n",
    "    y = softmax(a2)\n",
    "    # Backprop\n",
    "    delta2 = softmaxp(a2, -d/y)\n",
    "    delta1 = relup(a1, W2.T.dot(delta2))\n",
    "    W2 = W2 - gamma*delta2.dot(relu(a1).T)\n",
    "    b2 = b2 - gamma*delta2.sum(axis=1).reshape(No,1)\n",
    "    W1 = W1 - gamma*delta1.dot(x.T)\n",
    "    b1 = b1 - gamma*delta1.sum(axis=1).reshape(Nh,1)\n",
    "    \n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net = update_shallow(x,d,net,gamma)\n",
    "        y = forwardprop_shallow(x,net)\n",
    "        l = eval_loss(y,d)\n",
    "        err = eval_perfs(y, lbl)\n",
    "        print(\"Iteration #\"+str(t+1)+\": Error = \"+str(err)+\", Loss = \"+str(l)+'.')\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Error = 0.8689333333333333, Loss = 0.24246551027255084.\n",
      "Iteration #2: Error = 0.8506833333333333, Loss = 0.23383431669776872.\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain,dtrain,netinit,T=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Error = 0.8689333333333333, Loss = 0.24246551027255084.\n",
      "Iteration #2: Error = 0.8506833333333333, Loss = 0.23383431669776872.\n",
      "Iteration #3: Error = 0.7326166666666667, Loss = 0.20922325651968723.\n",
      "Iteration #4: Error = 0.6673333333333333, Loss = 0.19970592388194303.\n",
      "Iteration #5: Error = 0.6174, Loss = 0.19173705965929722.\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain,dtrain,netinit,T=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Error = 0.8689333333333333, Loss = 0.24246551027255084.\n",
      "Iteration #2: Error = 0.8506833333333333, Loss = 0.23383431669776872.\n",
      "Iteration #3: Error = 0.7326166666666667, Loss = 0.20922325651968723.\n",
      "Iteration #4: Error = 0.6673333333333333, Loss = 0.19970592388194303.\n",
      "Iteration #5: Error = 0.6174, Loss = 0.19173705965929722.\n",
      "Iteration #6: Error = 0.5543, Loss = 0.18452011963512463.\n",
      "Iteration #7: Error = 0.5255666666666666, Loss = 0.1777871288440342.\n",
      "Iteration #8: Error = 0.4755, Loss = 0.17148147461961294.\n",
      "Iteration #9: Error = 0.46395, Loss = 0.16555088497154002.\n",
      "Iteration #10: Error = 0.42183333333333334, Loss = 0.16000507081786172.\n",
      "Iteration #11: Error = 0.4242166666666667, Loss = 0.15476590673692553.\n",
      "Iteration #12: Error = 0.3836833333333333, Loss = 0.149896827340926.\n",
      "Iteration #13: Error = 0.39525, Loss = 0.14530072242716038.\n",
      "Iteration #14: Error = 0.3557666666666667, Loss = 0.14116414604658936.\n",
      "Iteration #15: Error = 0.37755, Loss = 0.13729381703466625.\n",
      "Iteration #16: Error = 0.33975, Loss = 0.13407526769330755.\n",
      "Iteration #17: Error = 0.36998333333333333, Loss = 0.1310908038119106.\n",
      "Iteration #18: Error = 0.34195, Loss = 0.12908259079420084.\n",
      "Iteration #19: Error = 0.3698666666666667, Loss = 0.12696635868988626.\n",
      "Iteration #20: Error = 0.35051666666666664, Loss = 0.1258906186700285.\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain,dtrain,netinit,T=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Error = 0.8689333333333333, Loss = 0.24246551027255084.\n",
      "Iteration #2: Error = 0.8506833333333333, Loss = 0.23383431669776872.\n",
      "Iteration #3: Error = 0.7326166666666667, Loss = 0.20922325651968723.\n",
      "Iteration #4: Error = 0.6673333333333333, Loss = 0.19970592388194303.\n",
      "Iteration #5: Error = 0.6174, Loss = 0.19173705965929722.\n",
      "Iteration #6: Error = 0.5543, Loss = 0.18452011963512463.\n",
      "Iteration #7: Error = 0.5255666666666666, Loss = 0.1777871288440342.\n",
      "Iteration #8: Error = 0.4755, Loss = 0.17148147461961294.\n",
      "Iteration #9: Error = 0.46395, Loss = 0.16555088497154002.\n",
      "Iteration #10: Error = 0.42183333333333334, Loss = 0.16000507081786172.\n",
      "Iteration #11: Error = 0.4242166666666667, Loss = 0.15476590673692553.\n",
      "Iteration #12: Error = 0.3836833333333333, Loss = 0.149896827340926.\n",
      "Iteration #13: Error = 0.39525, Loss = 0.14530072242716038.\n",
      "Iteration #14: Error = 0.3557666666666667, Loss = 0.14116414604658936.\n",
      "Iteration #15: Error = 0.37755, Loss = 0.13729381703466625.\n",
      "Iteration #16: Error = 0.33975, Loss = 0.13407526769330755.\n",
      "Iteration #17: Error = 0.36998333333333333, Loss = 0.1310908038119106.\n",
      "Iteration #18: Error = 0.34195, Loss = 0.12908259079420084.\n",
      "Iteration #19: Error = 0.3698666666666667, Loss = 0.12696635868988626.\n",
      "Iteration #20: Error = 0.35051666666666664, Loss = 0.1258906186700285.\n",
      "Iteration #21: Error = 0.36735, Loss = 0.12375517622933878.\n",
      "Iteration #22: Error = 0.34768333333333334, Loss = 0.12220061205869914.\n",
      "Iteration #23: Error = 0.35231666666666667, Loss = 0.11885046866238345.\n",
      "Iteration #24: Error = 0.3296, Loss = 0.11602941692641097.\n",
      "Iteration #25: Error = 0.32845, Loss = 0.11212134323225155.\n",
      "Iteration #26: Error = 0.3058166666666667, Loss = 0.10894056469191614.\n",
      "Iteration #27: Error = 0.30216666666666664, Loss = 0.10547509005222848.\n",
      "Iteration #28: Error = 0.2860333333333333, Loss = 0.10257210697992973.\n",
      "Iteration #29: Error = 0.2798, Loss = 0.09969469328232711.\n",
      "Iteration #30: Error = 0.2697, Loss = 0.09725539606281544.\n",
      "Iteration #31: Error = 0.2623333333333333, Loss = 0.09487581002653843.\n",
      "Iteration #32: Error = 0.25721666666666665, Loss = 0.09284282413922401.\n",
      "Iteration #33: Error = 0.24791666666666667, Loss = 0.09080601539223322.\n",
      "Iteration #34: Error = 0.24753333333333333, Loss = 0.08911951009124389.\n",
      "Iteration #35: Error = 0.23768333333333333, Loss = 0.08731433261277546.\n",
      "Iteration #36: Error = 0.2388, Loss = 0.08589407030620151.\n",
      "Iteration #37: Error = 0.22911666666666666, Loss = 0.08429929108322912.\n",
      "Iteration #38: Error = 0.23191666666666666, Loss = 0.0830770113673849.\n",
      "Iteration #39: Error = 0.22201666666666667, Loss = 0.08163501377034374.\n",
      "Iteration #40: Error = 0.22593333333333335, Loss = 0.0805529972788737.\n",
      "Iteration #41: Error = 0.21521666666666667, Loss = 0.07922569983840225.\n",
      "Iteration #42: Error = 0.22033333333333333, Loss = 0.07824606011333596.\n",
      "Iteration #43: Error = 0.2094, Loss = 0.07701003509039564.\n",
      "Iteration #44: Error = 0.21511666666666668, Loss = 0.076108648614764.\n",
      "Iteration #45: Error = 0.20423333333333332, Loss = 0.07496044817195067.\n",
      "Iteration #46: Error = 0.20986666666666667, Loss = 0.0741229587288851.\n",
      "Iteration #47: Error = 0.19955, Loss = 0.07304433867032206.\n",
      "Iteration #48: Error = 0.2043, Loss = 0.07225510865010407.\n",
      "Iteration #49: Error = 0.19448333333333334, Loss = 0.07124508957209172.\n",
      "Iteration #50: Error = 0.19875, Loss = 0.07049759277846887.\n",
      "Iteration #51: Error = 0.1896, Loss = 0.06955754791446371.\n",
      "Iteration #52: Error = 0.19295, Loss = 0.06884870455319093.\n",
      "Iteration #53: Error = 0.18481666666666666, Loss = 0.06797311928875545.\n",
      "Iteration #54: Error = 0.18788333333333335, Loss = 0.06729804643724352.\n",
      "Iteration #55: Error = 0.18126666666666666, Loss = 0.06647878483148742.\n",
      "Iteration #56: Error = 0.18315, Loss = 0.06583469529054035.\n",
      "Iteration #57: Error = 0.17765, Loss = 0.06507414011623723.\n",
      "Iteration #58: Error = 0.17828333333333332, Loss = 0.06445928708287756.\n",
      "Iteration #59: Error = 0.1738, Loss = 0.06375268724193484.\n",
      "Iteration #60: Error = 0.17346666666666666, Loss = 0.06316817495513453.\n",
      "Iteration #61: Error = 0.17053333333333334, Loss = 0.0625101450753382.\n",
      "Iteration #62: Error = 0.16936666666666667, Loss = 0.06195392463332914.\n",
      "Iteration #63: Error = 0.16683333333333333, Loss = 0.0613437125545442.\n",
      "Iteration #64: Error = 0.16606666666666667, Loss = 0.06081656136067446.\n",
      "Iteration #65: Error = 0.1639, Loss = 0.06024931148146415.\n",
      "Iteration #66: Error = 0.1624, Loss = 0.059752704293422915.\n",
      "Iteration #67: Error = 0.16135, Loss = 0.059224155602603606.\n",
      "Iteration #68: Error = 0.15908333333333333, Loss = 0.058753168735404045.\n",
      "Iteration #69: Error = 0.15846666666666667, Loss = 0.05826021745742512.\n",
      "Iteration #70: Error = 0.1564, Loss = 0.057814699270669796.\n",
      "Iteration #71: Error = 0.15588333333333335, Loss = 0.05735410907775695.\n",
      "Iteration #72: Error = 0.1538, Loss = 0.05693020700420179.\n",
      "Iteration #73: Error = 0.15393333333333334, Loss = 0.05649775035043799.\n",
      "Iteration #74: Error = 0.1512, Loss = 0.05609544376879602.\n",
      "Iteration #75: Error = 0.15191666666666667, Loss = 0.055689465003108965.\n",
      "Iteration #76: Error = 0.14903333333333332, Loss = 0.055309159699184665.\n",
      "Iteration #77: Error = 0.15005, Loss = 0.05492762874814496.\n",
      "Iteration #78: Error = 0.14706666666666668, Loss = 0.05456801792472956.\n",
      "Iteration #79: Error = 0.14838333333333334, Loss = 0.05420800972557882.\n",
      "Iteration #80: Error = 0.14508333333333334, Loss = 0.053867037712834735.\n",
      "Iteration #81: Error = 0.14623333333333333, Loss = 0.05352719313720095.\n",
      "Iteration #82: Error = 0.14345, Loss = 0.053203529714116245.\n",
      "Iteration #83: Error = 0.14478333333333335, Loss = 0.05288229243103873.\n",
      "Iteration #84: Error = 0.1417, Loss = 0.052575459927324174.\n",
      "Iteration #85: Error = 0.14315, Loss = 0.05227121422518598.\n",
      "Iteration #86: Error = 0.1404, Loss = 0.0519797325482338.\n",
      "Iteration #87: Error = 0.14146666666666666, Loss = 0.051691450111367734.\n",
      "Iteration #88: Error = 0.13921666666666666, Loss = 0.051414322964259025.\n",
      "Iteration #89: Error = 0.1401, Loss = 0.05114137244264674.\n",
      "Iteration #90: Error = 0.13786666666666667, Loss = 0.05087787299433165.\n",
      "Iteration #91: Error = 0.13886666666666667, Loss = 0.05061874145612033.\n",
      "Iteration #92: Error = 0.13655, Loss = 0.050367417689630226.\n",
      "Iteration #93: Error = 0.13765, Loss = 0.050120927233511595.\n",
      "Iteration #94: Error = 0.13543333333333332, Loss = 0.04988135456987486.\n",
      "Iteration #95: Error = 0.13665, Loss = 0.0496467081684577.\n",
      "Iteration #96: Error = 0.13456666666666667, Loss = 0.04941822267584724.\n",
      "Iteration #97: Error = 0.13563333333333333, Loss = 0.04919400298813968.\n",
      "Iteration #98: Error = 0.13338333333333333, Loss = 0.04897534601240464.\n",
      "Iteration #99: Error = 0.1345, Loss = 0.04876074437414004.\n",
      "Iteration #100: Error = 0.13261666666666666, Loss = 0.04855127250304101.\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain,dtrain,netinit,T=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, converges to 13.26% error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest, ltest=MNISTtools.load(dataset='testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(xtest.shape)\n",
    "print(ltest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing dataset has 10,000 items of dimension 784, like training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.1258, loss: 0.04621737992507283\n"
     ]
    }
   ],
   "source": [
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)\n",
    "pred = forwardprop_shallow(xtest,nettrain)\n",
    "loss = eval_loss(pred,dtest)\n",
    "err = eval_perfs(pred,ltest)\n",
    "print(\"Test error: \" + str(err)+\", loss: \" +str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test error is similar error rate to the training error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: Error = 0.08661666666666666, loss = 0.0300963452764584\n",
      "Epoch #2: Error = 0.066, loss = 0.023243066848732646\n",
      "Epoch #3: Error = 0.06376666666666667, loss = 0.021502988627834563\n",
      "Epoch #4: Error = 0.0532, loss = 0.017809165410952062\n",
      "Epoch #5: Error = 0.04175, loss = 0.014805063819567477\n"
     ]
    }
   ],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    N = x.shape[1]\n",
    "    NB = int((N+B-1)/B)\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for l in range(NB):\n",
    "            minibatch_indices = shuffled_indices[B*l:min(B*(l+1), N)]\n",
    "            mnbtch_idx = minibatch_indices # to shorten next line for PDF\n",
    "            net = update_shallow(x[:,mnbtch_idx],d[:,mnbtch_idx],net,gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        loss = eval_loss(y,d)\n",
    "        err = eval_perfs(y,lbl)\n",
    "        print(\"Epoch #\"+str(t+1)+\": Error = \"+str(err)+\", loss = \"+str(loss))\n",
    "    return net\n",
    "\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much faster convergence, much better error rate on training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.0461, loss: 0.015902600521269197\n"
     ]
    }
   ],
   "source": [
    "pred=forwardprop_shallow(xtest,netminibatch)\n",
    "loss=eval_loss(pred,dtest)\n",
    "err=eval_perfs(pred,ltest)\n",
    "print(\"Test error: \" + str(err)+\", loss: \" +str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly better results for mini-batch in much less time than the usual backprop, indeed reached 95.4% test accuracy compared to 87.3% for regular descent. It could be that regular back prop led to overfitting due to the repeated testing of the same data points. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
